#!/usr/local/bin/perl -w
#
# Parse weblogs for webalizer
#

use strict;
use File::Basename;
use File::Path;
use FindBin;
use POSIX;
use lib "$FindBin::RealBin/../../lib";
use ariba::Ops::Startup::Common;
use ariba::Ops::Startup::Apache;
use ariba::Ops::ProductAPIExtensions;
use ariba::rc::InstalledProduct;
use ariba::rc::Utils;
use ariba::rc::CipherStore;
use ariba::rc::Passwords;
use ariba::monitor::misc;
use ariba::util::Simplefind;

my $vers = '$Id: //ariba/services/monitor/bin/ws/parse-weblogs#6 $';

# debug levels
my $debug	= 0;      # debug level
$main::quiet	= 1;      # silence transferFromSrcToDest output?

# for webalizer linking to libpng/gd/jpeg
$ENV{ sharedLibEnvName() } = '/usr/local/lib';
$ENV{'PATH'} = '/usr/local/bin:' . $ENV{'PATH'};

# set up some defaults
my $me = ariba::rc::InstalledProduct->new();
my $service = $me->service();
chomp(my $bin = $me->installDir() . "/bin/" . lc(`uname -s`));

#defaults
my $DEFAULT_TMPDIRBASE    = ariba::Ops::Startup::Common::tmpdir()        . "/webstats";
my $DEFAULT_OUTPUTDIRBASE = ariba::monitor::misc::autoGeneratedDocsDir() . "/webstats";
my $DEFAULT_WEBALIZER  = "$bin/webalizer";
my $DEFAULT_ZMERGELOG  = "$bin/zmergelog";
my $DEFAULT_CONFIGFILE = $me->configDir() . "/webalizer.conf";

sub main {
	my ($tmpDirBase, $outputDirBase, $webalizer, $configFile, $zmergelog, $customLogDir, $startAt);
	my @productNames = ();

	while (my $arg = shift(@ARGV)) {
		if ($arg =~ m/^-d/o) { $debug = 1 }
		if ($arg =~ m/^-h/o) { usage() }
		if ($arg =~ m/^-t/o) { $tmpDirBase = shift(@ARGV) }
		if ($arg =~ m/^-o/o) { $outputDirBase = shift(@ARGV) }
		if ($arg =~ m/^-w/o) { $webalizer = shift(@ARGV) }
		if ($arg =~ m/^-c/o) { $configFile = shift(@ARGV) }
		if ($arg =~ m/^-z/o) { $zmergelog = shift(@ARGV) }
		if ($arg =~ m/^-l/o) { $customLogDir = shift(@ARGV) }
		if ($arg =~ m/^-s/o) { $startAt = shift(@ARGV) }
		if ($arg !~ m/^-/o ) { push(@productNames, $arg); next; }
	}

	$webalizer  ||= $DEFAULT_WEBALIZER;
	$zmergelog  ||= $DEFAULT_ZMERGELOG;

	$tmpDirBase     ||= $DEFAULT_TMPDIRBASE;
	$outputDirBase  ||= $DEFAULT_OUTPUTDIRBASE;
	$configFile		||= $DEFAULT_CONFIGFILE;


	# perform some sanity checking on args
	usage("webalizer not found or not executable") unless (-x $webalizer);
	usage("zmergelog not found or not executable") unless (-x $zmergelog);

	usage("Could not find webalizer config, $configFile") unless (-e $configFile);

	if ($startAt) {
		if ($startAt =~ m/(\d{4})-(\d{2})-(\d{2})/) {
			$startAt = POSIX::mktime(0, 0, 0, $3, ($2 - 1), ($1-1900), 0, 0, (localtime())[8]);
		} else {
			usage("Format for -s is YYYY-MM-DD");
		}
	}

	# sometimes files were unsuccesfully compressed on webserver, and downloaded as-is
	# zmergelog only accepts compressed files as arguments, so we will gzip these
	# if necessary
	#
	# here, just figure out where gzip is for later
	my $gzip = gzipCmd() || die "could not find gzip program\n";


	my $today = POSIX::mktime(0, 0, 0, (localtime())[3,4,5], 0, 0, (localtime())[8]);

	# do all ws-type products if none were specified
	if (!scalar(@productNames)) {
		@productNames = ("ws", "aesws", "ssws");
	}

	for my $productName (@productNames) {
		if (!ariba::rc::InstalledProduct->isInstalled($productName, $service)) {
			warn "$productName product not installed for $service\n" if $debug;
			next;
		}

		my $tmpDir     = "$tmpDirBase/$productName";
		my $outputDir  = "$outputDirBase/$productName";

		unless (-d $tmpDir) {
			mkdirRecursively([$tmpDir], 0, 0777);
			die "fatal: Unable to create $tmpDir!  Exiting.\n" unless (-d $tmpDir);
		}

		unless (-d $outputDir) {
			mkdirRecursively([$outputDir], 0, 0755);
			die "fatal: Unable to create $outputDir!  Exiting.\n" unless (-d $outputDir);
		}

		my $perProductStartAt = $startAt;
		# find out when last run if not specified
		unless ($perProductStartAt) {
			if (open(IN, "<$outputDir/webalizer.current")) {
				my ($lastRunYear, $lastRunMonth, $lastRunDay);
				while (my $line = <IN>) {
					next if $line =~ /^#/;
					($lastRunYear, $lastRunMonth, $lastRunDay) = (split(/\s+/, $line))[0,1,2];
					last;	# all we care about is first config line
				}
				close IN;

				$perProductStartAt = POSIX::mktime(0, 0, 0, ($lastRunDay + 1), ($lastRunMonth - 1), ($lastRunYear-1900), 0, 0, (localtime())[8]);
			}

			# if we couldn't figure it out, start a week ago.
			# we delete logs 7 days old anyway
			unless ($perProductStartAt) {
				$perProductStartAt = $today - (7 * 24 * 60 * 60);
			}
		}


		my $ws = ariba::rc::InstalledProduct->new($productName, $service);

		my $webserverRole = $ws->webserverRole();

		my $serviceHost = $ws->default('servicehost') || die "error: could not get service host for $productName\n";
		my $servicePort = $ws->default('WebServerHTTPPort') || "";
		my $logDir = $customLogDir || 
			ariba::Ops::Startup::Apache::logDirForRoleHostPort($webserverRole , $serviceHost, $servicePort);
		
		my @webMachines = $ws->hostsForRoleInCluster($webserverRole, $ws->currentCluster());
		my $wsUsername  = $ws->deploymentUser();

		# password for wsUser
		my $cipherStore = ariba::rc::CipherStore->new($service, $wsUsername);
		my $password = $cipherStore->valueForName($wsUsername);

		if (!$password and -t STDIN) {
			ariba::rc::Passwords::initialize($service);
			$password = ariba::rc::Passwords::lookup($wsUsername);
		}

		unless($password) {
			warn "ERROR: could not determine password for $wsUsername in $service!\n";
			next;
		}

		if ($debug) {
			print "debug: tmpDir = $tmpDir\n";
			print "debug: outputDir = $outputDir\n";
			print "debug: webalizer = $webalizer\n";
			print "debug: configFile = $configFile\n";
			print "debug: zmergelog = $zmergelog\n";
			print "debug: logDir = $logDir\n";
			print "debug: servicehost = $serviceHost\n";
			print "debug: webMachines = ", join(", ", @webMachines), "\n";
			print "debug: wsUser = $wsUsername\n";
			print "debug: startAt = $perProductStartAt (" . scalar(localtime($perProductStartAt)) . ")\n";
			print "debug: today = $today (" . scalar(localtime($today)) . "\n";
		}

		# when using webalizer in incremental mode, you *must* feed it
		# the logs in chronological order.  here use unixtime to do this right
		# and POSIX::strfdate to get the filename (httplog names them based on this 
				# function)
		my @logsToFetch = ();
		for (my $unixTime = $perProductStartAt; $unixTime < $today; $unixTime += (24 * 60 * 60)) {
			my $filename = POSIX::strftime("access.%Y.%m.%d", localtime($unixTime));
			print "debug: $unixTime => " . scalar(localtime($unixTime)) . " => $filename\n" if ($debug);
			push(@logsToFetch, $filename);
		}

		unless (@logsToFetch) {
			print "debug: no logs to fetch for product $productName\n" if $debug;
			next;
		}

		print "debug: fetching ", scalar(@logsToFetch), " logs\n" if $debug;

		# begin main loop
		foreach my $logfileName (@logsToFetch) {
			print "debug: Fetching $logfileName from all webservers\n" if ($debug);
			my @logFilesFetched;
			foreach my $webserver (@webMachines) {
				print "debug: Working on $webserver\n" if ($debug);

				# 
				# - download logfiles into tmpDir
				#
				# - make sure these are named uniquely by host, otherwise they
				#   will have the same names and clobber each other
				# - keep an array of files actually downloaded, for later use
				#
				# it is possible for logfiles to not be gzipped, so we must check
				# for the uncompressed version if the gzipped one is not found.
				# we must also compress it afterwards to be consistent for zmergelog
				#

				fetchFile("$logDir/$logfileName.gz", $webserver, "$tmpDir/$webserver-$logfileName.gz", $wsUsername, $password);

				if (-e "$tmpDir/$webserver-$logfileName.gz") {
					push(@logFilesFetched, "$tmpDir/$webserver-$logfileName.gz");
				} else {
					fetchFile("$logDir/$logfileName", $webserver, "$tmpDir/$webserver-$logfileName", $wsUsername, $password);
					if (-e "$tmpDir/$webserver-$logfileName") {
						if (system("$gzip $tmpDir/$webserver-$logfileName")) {
							die "problem running $gzip\n";
						}
						push(@logFilesFetched, "$tmpDir/$webserver-$logfileName.gz");
					} else {
						warn "unable to fetch $logfileName from $webserver!  skipping..\n";
						next;
					}
				}
			}

			#
			# now we must merge logfiles and parse with webalizer.  This is done via piped
			# processes, eliminating the need for more diskspace and IO.
			#
			# note: on a busy day, all 4 logfiles (gzipped) take about 10 megs.  These are removed
			# after parsing.
			#

			if (@logFilesFetched) {
				my $command = "$zmergelog " 
					. join(' ', @logFilesFetched) 
					. "| $webalizer -n $serviceHost -c $configFile -o $outputDir";

				print "debug: running $command\n" if $debug;
				if (system($command)) {
					die "return status fail from \"$command\"\n";
				}

				print "unlinking: ", join(", ", @logFilesFetched), "\n" if $debug;
				unlink(@logFilesFetched);	# get a rid of these now
			} else {
				warn "nothing to do for $logfileName\n";
			}

		}

		my $ignoreFile = "$outputDir/" . ariba::util::Simplefind->ignoreFileName();
		open(IGNORE, ">$ignoreFile");
		close(IGNORE);
	}

	exit(0);
}

sub fetchFile {
	# fetch files, needs full path to filename, the server, and full path to output
	my $fullPathToSrc = shift;
	my $server = shift;
	my $fullPathToDest = shift;
	my $username = shift;
	my $password = shift;

	my $srcRoot = dirname($fullPathToSrc);
	my $srcFile = basename($fullPathToSrc);

	my $destRoot = dirname($fullPathToDest);
	my $destFile = basename($fullPathToDest);

	# set to undef to see rsync output
	my $outputRef = [];

	my $ret = ariba::rc::Utils::transferFromSrcToDestNoCheck(
		$server,
		$username,
		$srcRoot,
		$srcFile,

		undef,
		undef,
		$destRoot,
		$srcFile,

		0,0,0,

		$password,
		$outputRef,
	);

	print "fetchFile: Fetching $srcRoot/$srcFile to $destRoot/$srcFile\n" if $debug;
	
	if ($ret) {
		# this is a hack.  transferSrcToDest automatically puts a /* at the end of the src if src and dest aren't named
		# the same.. but we need them named separately because we are going to merge 4 like-named files
		rename("$destRoot/$srcFile", $fullPathToDest);	
	} else {
		warn "transferFromSrcToDest failed\n";
	}
}

sub usage {
	print "error: $_[0]\n" if (@_);
	print "$0 (version $vers)\n";
	print "usage: $0 [<args>] [ <ws-product>  ... ]\n";
	print "   [-t]     use this tmp directory for weblogs (need ~10M)\n";
	print "   [-o]     base dir where to output webstats (default $DEFAULT_OUTPUTDIRBASE)\n";
	print "   [-w]     path to webalizer binary (default $DEFAULT_WEBALIZER)\n";
	print "   [-z]     path to zmergelog binary (default $DEFAULT_ZMERGELOG)\n";
	print "   [-l]     remote path to logfiles on webservers (default /var/log/apache/\$serviceHost)\n";
	print "   [-c]     path to webalizer.conf (default $DEFAULT_CONFIGFILE)\n";
	print "   [-d]     run with debugging\n";
	print "   [-h]     show usage and exit\n";
	print "   [-s]     specify in YYYY-MM-DD when to start\n";
	print "\n";
	print "   <ws-product>  (optional) one or more web server products (e.g. ws, aesws, etc.).\n";
	print "                 If no products are specified, all web server products will be done\n";

	exit(1);
}

main();

__END__

